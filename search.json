[{"title":"Torch如何更新参数","url":"/2025/07/30/how-torch-update-params/","content":"Torch如何更新参数\n基础知识\n1. 链式法则\n链式法则是微积分中的一个重要定理，它描述了复合函数的导数如何计算。\n链式法则的公式如下：\ndydx=dydududx\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}\ndxdy​=dudy​dxdu​\n其中，yyy是函数f(x)f(x)f(x)的输出，xxx是函数f(x)f(x)f(x)的输入，uuu是函数f(x)f(x)f(x)的中间变量。\n链式法则的应用非常广泛，例如在神经网络中，我们可以使用链式法则来计算损失函数对网络参数的导数。\n2. 反向传播\n反向传播是神经网络中常用的一种计算梯度的方法。\n反向传播的基本思想是，从输出层开始，沿着网络的反向路径，计算每个节点的梯度。\n反向传播的公式如下：\n∂L∂wij=∂L∂y∂y∂wij\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w_{ij}}\n∂wij​∂L​=∂y∂L​∂wij​∂y​\n其中，LLL是损失函数，wijw_{ij}wij​是网络参数，yyy是网络输出。\n3. 梯度下降\n梯度下降是一种常用的优化算法，用于最小化损失函数。\n梯度下降的基本思想是，从一个初始点开始，沿着负梯度方向移动，直到达到局部最小值。\n梯度下降的公式如下：\nwk+1=wk−α∂L∂wkw_{k+1} = w_{k} - \\alpha \\frac{\\partial L}{\\partial w_{k}}\nwk+1​=wk​−α∂wk​∂L​\n其中，wkw_{k}wk​是第kkk次迭代的参数，α\\alphaα是学习率。\n基本流程\n1. 前向传播\n前向传播是神经网络中计算输出的过程。\n前向传播的基本思想是，从输入层开始，沿着网络的正向路径，计算每个节点的输出。\n前向传播的公式如下：\ny=f(w1x1+w2x2+...+wnxn)y = f(w_{1} x_{1} + w_{2} x_{2} + ... + w_{n} x_{n})\ny=f(w1​x1​+w2​x2​+...+wn​xn​)\n其中，yyy是网络输出， wiw_{i}wi​是第iii个网络参数， xix_{i}xi​是第iii个输入。\n在Torch中，我们可以使用以下代码来实现前向传播：\nimport torchimport torch.nn as nn# 定义网络class Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        self.fc1 = nn.Linear(10, 20)        self.fc2 = nn.Linear(20, 10)    def forward(self, x):        x = self.fc1(x)        x = self.fc2(x)        return x# 定义输入x = torch.randn(1, 10)# 定义网络net = Net()# 前向传播y = net(x)\n在前向传播过程中会按照计算图依次计算每个节点的输出，并记录。\n2. 计算损失函数\n计算损失函数是神经网络中计算误差的过程。\n计算损失函数的基本思想是，将网络输出与真实标签进行比较，计算误差。\n假设损失函数为$$L(y, t)$$，其中yyy是网络输出，ttt是真实标签。\n在Torch中，我们可以使用以下代码来计算损失函数：\n# 定义损失函数criterion = nn.MSELoss()# 定义真实标签t = torch.randn(1, 10)# 计算损失函数loss = criterion(y, t)# 打印损失函数print(loss)\n在计算损失函数过程中会按照计算图依次计算每个节点的输出，并记录。\n3. 反向传播\n反向传播是神经网络中计算梯度的过程。\n反向传播的基本思想是，从输出层开始，沿着网络的反向路径，计算每个节点的梯度。\n反向传播的公式如下：\n∂L∂wij=∂L∂y∂y∂wij\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w_{ij}}\n∂wij​∂L​=∂y∂L​∂wij​∂y​\n其中，LLL是损失函数，wijw_{ij}wij​是网络参数，yyy是网络输出。\n多层网络中的反向传播可以使用链式法则来计算：\n假设网络有LLL层，第lll层的输出为yly_{l}yl​ ，第lll层的参数为wlw_{l}wl​，第l+1l+1l+1层的输出为yl+1y_{l+1}yl+1​，第l+1l+1l+1层的参数为wl+1w_{l+1}wl+1​，则反向传播的公式如下：\n∂L∂wl=∂L∂yl+1∂yl+1∂yl∂yl∂wl\\frac{\\partial L}{\\partial w_{l}} = \\frac{\\partial L}{\\partial y_{l+1}} \\frac{\\partial y_{l+1}}{\\partial y_{l}} \\frac{\\partial y_{l}}{\\partial w_{l}}\n∂wl​∂L​=∂yl+1​∂L​∂yl​∂yl+1​​∂wl​∂yl​​\n在Torch中，我们可以使用以下代码来实现反向传播：\n# 反向传播loss.backward()\n在反向传播过程中会按照计算图依次计算每个节点的梯度，并记录。\n4. 更新参数\n更新参数是神经网络中更新网络参数的过程。\n更新参数的基本思想是，根据梯度下降的公式，更新每个网络参数。\n梯度下降的公式如下：\nwk+1=wk−α∂L∂wkw_{k+1} = w_{k} - \\alpha \\frac{\\partial L}{\\partial w_{k}}\nwk+1​=wk​−α∂wk​∂L​\n其中，wkw_{k}wk​是第kkk次迭代的参数，α\\alphaα是学习率。\n在Torch中，我们可以使用以下代码来更新参数：\n# 更新参数optimizer.step()\n在更新参数过程中会按照计算图依次计算每个节点的梯度，并更新每个网络参数。\n总结\nTorch如何更新参数的基本流程如下：\n\n前向传播：从输入层开始，沿着网络的正向路径，计算每个节点的输出。\n计算损失函数：将网络输出与真实标签进行比较，计算误差。\n反向传播：从输出层开始，沿着网络的反向路径，计算每个节点的梯度。\n更新参数：根据梯度下降的公式，更新每个网络参数。\n\n","categories":["AI","基础知识"],"tags":["AI","torch","深度学习训练阶段"]},{"title":"默认测试文章","url":"/2025/07/29/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start\nCreate a new post\n$ hexo new &quot;My New Post&quot;\ntestsdfnujidfnbnfnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\nMore info: Writing\nRun server\n$ hexo server\nMore info: Server\nGenerate static files\n$ hexo generate\nMore info: Generating\nDeploy to remote sites\n$ hexo deploy\nMore info: Deployment\n","categories":["测试类别","日常","测试子类","日常1","测试子类1","test1"],"tags":["测试标签"]}]